---
title: "Econ 184b, Econometrics, Assignment 6"
output:
  html_document: default
  pdf_document: default
---

```{r,echo=F}
knitr::opts_chunk$set(message=F,warning=F)
```

```{r,echo=F,message=FALSE}
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(magrittr)
library(readr)
code_dir = dirname(rstudioapi::getActiveDocumentContext()$path)
knitr::opts_knit$set(root.dir=code_dir)
# setwd(code_dir)
```


The raw .rmd file for this assignment are available at: https://raw.githubusercontent.com/f1kidd/Econ184/main/Assignment/Assignment_5.Rmd

Note: For the math problems, you can either (1) type your solution and compile it with RMarkdown; (2) type your solution use another word-processing software and convert it into a PDF file; (3) write your solution on paper, scan it and make it into a legible PDF file. TAs can decide, at their discretion, that the submitted file is illegible and thus give zero credit to a question or the entire problem set. If you type your solutions (options 1 and 2 above), you will get **a bonus equal to 5% of your performance** on the problem set. No credit will be given if you only report the final answers without showing intermediate steps whenever appropriate. 

For the programming problems, you need to submit both the compiled pdf/html file and the RMarkdown (.rmd) file on LATTE. You will get **an additional bonus equal to 5% of your performance** if your RMarkdown file is completely reproducible with minimal altercation (install packages, etc.). That is, our team (or anyone else) should be able to recompile your Rmarkdown file and reach *the same* result. You need to explicitly write out your answers - just showing programming outputs will receive zero credit. 

You will be receiving a total of 15% bonus if you submit one single pdf/html compiled directly from Rmarkdown, along with the .rmd file. 



### Question 1 (10 points each): 

There are three kinds of candy in a huge candy bag: the M&M, the Godiva, and the Sugus. A random variable Y has the following probability distribution: Pr(Y = M&M) = p, Pr(Y = Godiva) = q, and Pr(Y = Sugus) = 1 - p - q. You reach into the bag and randomly grab n=100 pieces of candy from the bag: random variables are denoted $Y_1,Y_2,...,Y_n$.

a. Denote $N_1,N_2,N_3$ as the number of M&M, Godiva, and Sugus draws from the bag ($N_1+N_2+N_3=100$). Derive the likelihood function for the parameters p and q.

Answer: the likelihood function is given by the probability that a particular draw happens. The likelihood function is thus given as:

$$L = p^{N_1} * q^{N_2} * (1-p-q)^{N_3}$$

b. Suppose amongst these 100 pieces, there are 20 M&Ms, 30 Godivas, and 50 Sugus. Derive the maximum likelihood estimator for p and q.

The MLE estimator maximizes the log likelihood given the data: 

$$max_{p,q} \quad log(L)=N_1*log(p) + N_2*log(q) + N3*log(1-p-q)$$
Take the derivatives with respect to both p and q and set both to zero:
\begin{equation}
\begin{aligned}
\frac{\partial logL}{\partial p} & = \frac{N1}{p} - \frac{N3}{1-p-q}=0\\
\frac{\partial logL}{\partial q} & = \frac{N2}{q} - \frac{N3}{1-p-q}=0\\
\end{aligned}
\end{equation}

Solving the system of equations yield:

$$p=\frac{N_1}{N_1+N_2+N_3} \quad q=\frac{N_2}{N_1+N_2+N_3}$$

Plug in the numbers, we have p=0.2 and q=0.3. 

### Question 2: Who tends to have extramarital affairs? (5 points each)

This question uses a data set on Github ([affairs.csv](https://github.com/f1kidd/Econ184/blob/main/Data/affairs.csv?raw=true) and  [affairs_description.pdf](https://github.com/f1kidd/Econ184/blob/main/Data/affairs_description?raw=true)) from a 1969 survey filled out by readers of Psychology Today. The sample includes employed men and women who are married for the first time. We want to know if there are any significant relationships between the probability of having an extramarital affair and certain characteristics of married people.

a. Looking at the variables in the dataset, do you expect a relationship between extramarital affairs and any of the variables in the dataset? If yes, what sign(s) do you expect?

Answer: A number of variables could affect extramarital affairs. (+) denotes a potential positive relationship, (-) negative relationship; (?) uncertain relationship. 

gender(?); age(+); yearsmarried(+); children(-); religiousness(-); education(?); occupation(?); rating(-).

```{r}
url = "https://github.com/f1kidd/Econ184/blob/main/Data/affairs.csv?raw=true"
affairs = read_csv(url)
```


b. Estimate a linear probability model of affair (not affairs) on male, age, yearsmarried, kids, religiousness, education, and rating (all in one regression). Remember to use the heteroskedasticity robust standard errors.

```{r}
# abbadaba
library(sandwich)
library(lmtest)
lm1 = lm(affair ~ gender + age +yearsmarried+children+
           religiousness+education+rating,data=affairs)
coeftest(lm1,vcov=vcovHC)
```

c. Interpret the coefficient on "rating".

Answer: when the self-rated marriage rating increase by 1, the probability of having an extramarital affair decreases by 0.08%.

d. What is the expected probability of having an affair for Veronica, a 30-year-old woman who is a high school graduate, with no children, has been married for 3 years, with average religious level (= 3) and is very happy ( = 5) in her marriage?

```{r}
veronica = data.frame(gender="female",age=30,yearsmarried=3,
            education=12,children="no",religiousness=3,rating=5)
predict(lm1,veronica)
```

Veronica has a 1.6% probability of having an extramarital affair. 
e. Now consider a very religious woman, Katelyn, 25 years old, no children, who is happily married for a year and has 9 years of education. What is Katelynâ€™s expected probability of having an affair? What does this imply about the applicability of the linear probability model in this case?

Answer: The expected probability for Katelyn is -0.1. This clearly does no make sense: probability cannot be negative. That is one of the shortcomings with the LPM: it can produce negative predicted probabilities.

```{r}
katelyn = data.frame(gender="female",age=25,yearsmarried=1,
            education=9,children="no",religiousness=5,rating=5)
predict(lm1,katelyn)
```

f. Estimate the same regression as in (b) using a probit regression. Are the results similar as in (b) in terms of statistical significance of the coefficients?

Answer: the results are similar in terms of statistical significance.
```{r}
probit1 = glm(affair ~ gender + age +yearsmarried+children+
           religiousness+education+rating,data=affairs,
           family=binomial(link="probit"))
coeftest(probit1,vcov=vcovHC)
```


g. Repeat the calculation of predicted probabilities for Veronica and Katelyn.

Veronica has a 6% chance of having an affair. Katelyn has a 2.5% chance.
```{r}
pnorm(predict(probit1,veronica))
pnorm(predict(probit1,katelyn))
```
h. Estimate the same regression as in (b) using a logit regression.

Answer: See code below.
```{r}
logit1 = glm(affair ~ gender + age +yearsmarried+children+
           religiousness+education+rating,data=affairs,
           family=binomial(link="logit"))
coeftest(logit1,vcov=vcovHC)
```

i. Repeat the calculation of predicted probabilities for Veronica and Katelyn.

Veronica has a 6.6% chance of having an affair. Katelyn has a 3.3% chance.
```{r}
# abbadaba
1/(1+exp(-predict(logit1,veronica)))
1/(1+exp(-predict(logit1,katelyn)))
```


j. Which model appears to be most appropriate in this example? Explain.

Answer: There are a number of ways to assess the appropriateness of the model. A simple comparison of AIC (Akaike Information Criteria) reveals that probit and logit are almost identical. LPM has worse performance than both probit and logit. 

In this case, either probit or logit are appropriate.

Other criteria could include: prediction without kink (prefers logit/probit over LPM). Predictive accuracy (not required). 

```{r}
AIC(logit1)
AIC(probit1)
AIC(lm1)
```

## Question 3: Predicting Breast Cancer (5 points each)

This question uses the Wisconsin Breast Cancer dataset (available [here](https://raw.githubusercontent.com/f1kidd/Econ184/main/Data/breastcancer.csv)). For a documentation of the data set, please visit https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original). 

a. Our goal is to predict whether a patient's breast tumor is benign or malignant (cancer). Run a probit model, predict the class of the tumor using all features. Which of these factors point to a malignant breast tumor with 5% statistical significance?

Answer: The presence of clump thinkness, uniformaty of cell shape, marginal adhesion, bland chromatin, and mitoses all point to malignant breast tumor with 5% statistical significance. 

```{r}
library(lmtest)
library(sandwich)

url = "https://raw.githubusercontent.com/f1kidd/Econ184/main/Data/breastcancer.csv"
breastcancer = read_csv(url)
breastcancer %<>% mutate(Class=ifelse(as.character(Class)=="benign",0,1))

probit1 = glm(Class~.,data=breastcancer,family=binomial(link="probit"))
coeftest(probit1,vcov=vcovHC)
```

b. What is the in-sample predictive accuracy of the model?

Answer: 0.961.
```{r}
probit1_pred = as.numeric(predict(probit1)>0)
mean(probit1_pred==breastcancer$Class)
```

c. Cross-validate the above model using 10-fold cross validation. What is the out-of-sample predictive accuracy of the probit model?

Answer: this number will vary every time the CV routine runs. 
```{r}
require(boot)
cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)

cv_probit1 = cv.glm(data=breastcancer,glmfit=probit1,cost=cost,K=10)
1 - cv_probit1$delta[2]
```
d. Estimate a linear probability model with the same variable. What is the in-sample predictive accuracy of the model? (Note: it will be easier to use the *glm* function instead of the *lm* function to estimate the LPM. *glm* defaults to the OLS model identical to *lm*.)


```{r}
lm1 = glm(Class~.,data=breastcancer)
lm1_pred = as.numeric(predict(lm1)>0.5)
mean(lm1_pred == breastcancer$Class)
```

Answer: 0.954. 

e. Cross-validate the above model using 10-fold cross validation. What is the out-of-sample predictive accuracy of the linear probability model?

Answer: this model will vary every time the model runs. 
```{r}
cv_lm1 = cv.glm(data=breastcancer,glmfit=lm1,cost=cost,K=10)
1 - cv_lm1$delta[2]
```


